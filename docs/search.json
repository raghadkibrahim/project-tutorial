[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI For Lawyers",
    "section": "",
    "text": "Introduction\n\nThis book documents every step of the project so my dad can follow along — from data ingestion to evaluation.\n\n\n\n\n\n\n\nTip\n\n\n\nHow to read this book — The left sidebar lists parts and chapters chronologically. Each chapter shows the latest results; older runs are archived in the Progress appendix.\n\n\n\nProject repo: ?var:book.repo-url\nLatest build: Rendered on each push to main.\n\nThis project builds a legal‑AI stack around Arabic court verdicts. Objectives:\n\nClean and structure raw verdicts (facts, reasoning, verdict, statutes).\nBuild retrieval and analytics pipelines.\nEvaluate embeddings and LLMs for precedent search and drafting.\nShip a transparent, reproducible pipeline.\n\nWe use Python in most chapters; code is baked into the pages, with caches for reproducibility.\n\n\nDataset\nThis dataset contains comprehensive details about all verdicts generated by Dubai Courts judgements. It can be accessed from Dubai Pulse which is the city’s open data portal.\n\n\n\n\n\n\n\n\nVariable\nDescription\nType\n\n\n\n\ncase_subtype_code\nNumber to identify the type of a case\nint\n\n\ncase_year\nNumber reflect the year when the case is raised.\nint\n\n\ncase_serial_number\nNumber reflects the case serial based on the code and year.\nint\n\n\ndecision_number\nNumber to identify the judgment decision.\nint\n\n\njudgment_date\nVerdict date\nDATE\n\n\nrecitals\nJudgment Text\nstring\n\n\ninsertion_date\nInsertion date\nDATE\n\n\nlast_update_date\nlast update date\nDATE",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "project-1.html",
    "href": "project-1.html",
    "title": "Project I - Case Similarity Search Engine",
    "section": "",
    "text": "This part covers locating sources, licensing, schema mapping, and ingestion pipelines.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Project I - Case Similarity Search Engine"
    ]
  },
  {
    "objectID": "ch1-1.html",
    "href": "ch1-1.html",
    "title": "1  Explanation",
    "section": "",
    "text": "We describe source datasets, provenance, and license constraints.\n\nPrimary dataset: Dubai Court verdicts CSV (≈217k rows).\nLinked lookup: case_subtype_code → subtype labels (external dataset).\n\n\nWe track download URIs and hashes so we can prove provenance.",
    "crumbs": [
      "Project I - Case Similarity Search Engine",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Explanation</span>"
    ]
  },
  {
    "objectID": "ch1-2.html",
    "href": "ch1-2.html",
    "title": "Data Preprocessing",
    "section": "",
    "text": "import pandas as pd\nfrom pathlib import Path\nfrom datasets import load_dataset\n\nverdicts = load_dataset(\n    \"csv\",\n    data_files=\"hf://datasets/raghadkibrahim/dxb_court_data/Verdicts.csv\",\n    streaming=True,\n    split=\"train\",\n)\n\n/opt/anaconda3/envs/openai_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\nimport itertools\n\nsample = list(verdicts.take(5000)) \ndf = pd.DataFrame(sample)\n\ndf.head() \n\n\n\n\n\n\n\n\ncase_subtype_code\ncase_year\ncase_serial_number\ndecision_number\njudgment_date\nrecitals\ninsertion_date\nlast_update_date\n\n\n\n\n0\n1\n1992\n1306\n14\n01-08-2022\nبعد الاطلاع على الأوراق و سماع المرافعة؛؛؛    ...\n01-08-2022\nNone\n\n\n1\n11\n2016\n629\n22\n19-09-2022\nبعد سماع المرافعة الشفوية ومطالعة الاوراق:حيث ...\n19-09-2022\nNone\n\n\n2\n11\n2016\n1005\n18\n25-04-2024\nبعد مطالعة الملف الإلكتروني سماع المرافعة حيث ...\n25-04-2024\nNone\n\n\n3\n11\n2018\n634\n21\n30-05-2022\nبعد سماع المرافعة والاطلاع على الأوراق  : حيث ...\n30-05-2022\nNone\n\n\n4\n11\n2019\n1959\n16\n28-02-2022\nبعد سماع المرافعة ومطالعة الاوراق . حيث ان وقا...\n28-02-2022\nNone\n\n\n\n\n\n\n\n\ndef clean_text(text):\n    text = re.sub(r'\\s+', ' ', text)  # remove extra spaces\n    text = re.sub(r'[^\\w\\s]', '', text)  # remove punctuation\n    return text.strip()\n\n\nimport re\n\ndf['cleaned_recitals'] = df['recitals'].apply(clean_text)\n\ndf.head()\n\n\n\n\n\n\n\n\ncase_subtype_code\ncase_year\ncase_serial_number\ndecision_number\njudgment_date\nrecitals\ninsertion_date\nlast_update_date\ncleaned_recitals\n\n\n\n\n0\n1\n1992\n1306\n14\n01-08-2022\nبعد الاطلاع على الأوراق و سماع المرافعة؛؛؛    ...\n01-08-2022\nNone\nبعد الاطلاع على الأوراق و سماع المرافعة وحيث إ...\n\n\n1\n11\n2016\n629\n22\n19-09-2022\nبعد سماع المرافعة الشفوية ومطالعة الاوراق:حيث ...\n19-09-2022\nNone\nبعد سماع المرافعة الشفوية ومطالعة الاوراقحيث ا...\n\n\n2\n11\n2016\n1005\n18\n25-04-2024\nبعد مطالعة الملف الإلكتروني سماع المرافعة حيث ...\n25-04-2024\nNone\nبعد مطالعة الملف الإلكتروني سماع المرافعة حيث ...\n\n\n3\n11\n2018\n634\n21\n30-05-2022\nبعد سماع المرافعة والاطلاع على الأوراق  : حيث ...\n30-05-2022\nNone\nبعد سماع المرافعة والاطلاع على الأوراق حيث إن...\n\n\n4\n11\n2019\n1959\n16\n28-02-2022\nبعد سماع المرافعة ومطالعة الاوراق . حيث ان وقا...\n28-02-2022\nNone\nبعد سماع المرافعة ومطالعة الاوراق حيث ان وقائ...\n\n\n\n\n\n\n\n\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n\nembs = model.encode(df['cleaned_recitals'].astype(str).tolist(), batch_size=256, show_progress_bar=True)\n\n# Save to disk for FAISS/ScaNN",
    "crumbs": [
      "Project I - Case Similarity Search Engine",
      "Data Preprocessing"
    ]
  },
  {
    "objectID": "project-2.html",
    "href": "project-2.html",
    "title": "Project II",
    "section": "",
    "text": "We move from clean data → embeddings → retrieval → evaluation.",
    "crumbs": [
      "Project II"
    ]
  },
  {
    "objectID": "ch2-1.html",
    "href": "ch2-1.html",
    "title": "2  Chapter 2-1",
    "section": "",
    "text": "import re\nimport pandas as pd\n\n# example cleaner\ndef clean_ar(text):\n    if not isinstance(text, str):\n        return ''\n    text = re.sub(r\"[\\u0617-\\u061A\\u064B-\\u0652]\", \"\", text)  # remove diacritics\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\n# apply\n# df['cleaned_recitals'] = df['recitals'].map(clean_ar)",
    "crumbs": [
      "Project II",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Text Cleaning & Normalization</span>"
    ]
  },
  {
    "objectID": "ch2-4.html",
    "href": "ch2-4.html",
    "title": "5  Chapter 2.4",
    "section": "",
    "text": "We show prompt templates, red‑teaming, and guardrails for legal use.",
    "crumbs": [
      "Project II",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Drafting Assist & Risk Controls</span>"
    ]
  }
]