# Data Preprocessing {.unnumbered}

## Import Libraries
```{python}

#| echo: true
#| eval: true
#| fig-cap: "Basic row counts by year"
import pandas as pd
from pathlib import Path
from datasets import load_dataset
import numpy as np

```

## Load Dataset
```{python}
verdicts = load_dataset(
    "csv",
    data_files="hf://datasets/raghadkibrahim/dxb_court_data/Verdicts.csv",
    streaming=True,
    split="train",
)

```


## Show a sample
```{python}

#| echo: true
#| eval: true

import itertools

sample = list(verdicts.take(5000)) 
df = pd.DataFrame(sample)

df.head() 

```


## Clean `recitals` Column

We will start by ceating a function to clean the text in the `recitals` column.

``` {python}
#| echo: true
#| eval: true

def clean_text(text):
    text = re.sub(r'\s+', ' ', text)  # remove extra spaces
    text = re.sub(r'[^\w\s]', '', text)  # remove punctuation
    return text.strip()

```


Applying the function:

``` {python}
#| echo: true
#| eval: true
import re

df['cleaned_recitals'] = df['recitals'].apply(clean_text)

df.head()

```

## Create Semantic Embeddings

Embeddings turn text into numeric vectors that capture meaning. There are a few options we could choose from, but in this case we shall use `sentence_transformer` for its excellent multingual abilities.

``` {python}
import torch
from sentence_transformers import SentenceTransformer

device = "mps" if torch.backends.mps.is_available() else "cpu"

model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
model = model.to(device)

```

``` {python}

# Prepare texts 
texts = df["cleaned_recitals"].astype(str).tolist()

# Encode in chunks to control memory; keep batch_size modest on MPS
emb_chunks = []
for i in range(0, len(texts), 2000):  
    batch = texts[i:i+2000]
    embs = model.encode(
        batch,
        batch_size=64,                          # 32â€“128 is usually fine; tweak if you see OOM
        show_progress_bar=True,
        convert_to_numpy=True,
        normalize_embeddings=True         
    )
    emb_chunks.append(embs.astype("float32"))   # FAISS prefers float32
embeddings = np.vstack(emb_chunks)

```